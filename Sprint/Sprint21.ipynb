{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN():\n",
    "    def __init__(self, n_features, n_nodes, initializer=None, optimizer=None, activator1=None, activator2=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.activator1 = activator1\n",
    "        self.activator2 = activator2\n",
    "#         self.Wx = initializer.W(n_features, n_nodes)\n",
    "#         self.b = initializer.b(n_nodes)\n",
    "#         self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "        \n",
    "    def forward(self, x, h_in):\n",
    "        self.Z = np.dot(x, self.Wx) + np.dot(h_in, self.Wh) + self.b\n",
    "        self.h_out = self.activator1.forward(self.Z)\n",
    "        self.out = self.activator2.forward(self.h_out)\n",
    "        return self.out, self.h_out\n",
    "    \n",
    "    def backward(self, y_true, y_pred, dh_out):\n",
    "        dh_out = dh_out + self.activator2.backward(y_pred, y_true)\n",
    "        dZ = self.activator1.backward(dh_out)\n",
    "        dh_in, dWx, dWh, db = self.optimizer.backward(self, dZ)\n",
    "        return dh_in, dWx, dWh, db\n",
    "    \n",
    "    def update(self, dWx, dWh, db):\n",
    "        self.optimizer.update(self, dWx, dWh, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def backward(self, layer, dZ):\n",
    "        db = np.sum(dZ, axis=0)\n",
    "        dWx = np.dot(layer.x.T, dZ)\n",
    "        dx = np.dot(dZ, layer.Wx.T)\n",
    "        dWh = np.dot(layer.h.T, dZ)\n",
    "        dh = np.dot(dZ, layer.Wh.T)\n",
    "        return dh, dWx, dWh, db\n",
    "    \n",
    "    def update(self, layer, dWx, dWh, db):\n",
    "        layer.Wx -= self.lr * dWx\n",
    "        layer.Wh -= self.lr * dWh\n",
    "        layer.b -= self.lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        if Z.ndim == 2:\n",
    "            Z = Z.T\n",
    "            A = (np.exp(Z) / np.sum(np.exp(Z), axis=0)).T\n",
    "            return A\n",
    "        A = np.exp(Z) / np.sum(np.exp(Z))\n",
    "        return A\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "    \n",
    "    \n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        A = np.tanh(Z)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA, A):\n",
    "        return dA * np.square(1 - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleRNNClassifier():\n",
    "    def __init__(self, layer, epoch=3):\n",
    "        self.epoch = epoch\n",
    "        self.layer = layer\n",
    "        self.loss_train = []\n",
    "        self.loss_valid = []\n",
    "\n",
    "    def train(self, X, y, X_val=None, y_val=None):\n",
    "        # X shape: (batch_size, n_sequences, n_features)\n",
    "        n_sequences = X.shape[1]\n",
    "        self.out_list = []\n",
    "        self.hout_list = []\n",
    "        self.dh_list = []\n",
    "        self.dWx_list = []\n",
    "        self.dWh_list = []\n",
    "        self.db_list = []\n",
    "        \n",
    "        for _ in tqdm(range(self.epoch)):\n",
    "            \n",
    "            # feedforward\n",
    "            self.h = np.zeros((batch_size, n_nodes))\n",
    "            h_out = np.zeros((batch_size, n_nodes))\n",
    "            for n in range(n_sequences):\n",
    "                out, h_out = self.layer.forward(X[:,n,:], h_out)\n",
    "                self.out_list.append(out)\n",
    "                self.hout_list.append(h_out)\n",
    "                \n",
    "            # backward\n",
    "            dh = np.zers((batch_size, n_nodes))\n",
    "            for n in range(n_sequences):\n",
    "                dh, dWx, dWh, db = layer.backward(self.out_list[-n-1], dh)\n",
    "                self.dh_list.append(dh)\n",
    "                self.dWx_list.append(dWx)\n",
    "                self.dWh_list.append(dWh)\n",
    "                self.db_list.append(db)\n",
    "                \n",
    "            # update weight\n",
    "            layer.update(np.sum(dWx), np.sum(dWh), np.sum(db))\n",
    "#           self.loss_train.append(self.crossentropy(mini_y_train, fout))        \n",
    "        \n",
    "    def crossentropy(self, y, y_pred):\n",
    "        loss = -np.mean(np.sum(y*np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "    \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        out = X_test\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "tanh = Tanh()\n",
    "sgd = SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN(2, 4, None, sgd, tanh, softmax)\n",
    "rnn.Wx = w_x\n",
    "rnn.Wh = w_h\n",
    "rnn.b = b\n",
    "rnn.h = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cls = ScratchSimpleRNNClassifier(rnn, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 464.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.76188798, 0.76213958, 0.76239095, 0.76255841]]),\n",
       " array([[0.792209  , 0.8141834 , 0.83404912, 0.84977719]]),\n",
       " array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_cls.train(x, None)\n",
    "rnn_cls.hout_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
