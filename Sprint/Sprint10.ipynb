{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv():\n",
    "    def __init__(self, initializer, optimizer, in_channel=1, out_channel=1, kernel=(1,1), stride=(1,1), padding=0, to_nn=False):\n",
    "        self.stride = stride    # tuple\n",
    "        self.padding = padding  # int\n",
    "        self.kernel = kernel    # tuple\n",
    "        self.W = initializer.W(in_channel, out_channel, kernel[0], kernel[1])\n",
    "        self.b = initializer.b(out_channel)\n",
    "        self.optimizer = optimizer\n",
    "        self.to_nn = to_nn\n",
    "        self.Z = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.to_nn:\n",
    "            X = X.reshape(X.shape[0], 1, 1, X.shape[1])\n",
    "        #Xの次元は(batchsize, channel, height, width)\n",
    "        self.X = X\n",
    "        \n",
    "        #outputのshapeは 0:in_channel, 1:out_channel, 2:out_height, 3:out_width\n",
    "        self.batchsize, self.in_channel, self.in_h, self.in_w = X.shape\n",
    "        self.out_channel = len(self.b)\n",
    "        self.out_h = (self.in_h+2*self.padding-self.kernel[0])/self.stride[0] + 1        \n",
    "        self.out_w = (self.in_w+2*self.padding-self.kernel[1])/self.stride[1] + 1\n",
    "        if not self.out_w.is_integer() or not self.out_h.is_integer():\n",
    "            print('Check kernelsize, stride, padding')\n",
    "            print(f'output shape: ({self.batchsize}, {self.out_channel}, {self.out_h}, {self.out_w})')\n",
    "            return \n",
    "        else:\n",
    "            self.out_h, self.out_w = int(self.out_h), int(self.out_w)\n",
    "            #print(f'output shape: ({self.batchsize}, {self.out_channel}, {self.out_h}, {self.out_w})')\n",
    "        \n",
    "        if self.padding == 0:\n",
    "            self.X_pad = self.X.copy()\n",
    "        elif self.padding > 0:\n",
    "            self.X_pad = np.zeros((self.batchsize, self.in_channel,\n",
    "                                   self.in_h+2*self.padding, self.in_w+2*self.padding))\n",
    "            self.X_pad[:,:,self.padding:-self.padding, self.padding:-self.padding] += self.X\n",
    "        \n",
    "        self.Z = np.zeros((self.batchsize, self.out_channel, self.out_h, self.out_w))\n",
    "        sh, sw = self.stride\n",
    "        kh, kw = self.kernel\n",
    "        for b in range(self.batchsize):\n",
    "            for o_ch in range(self.out_channel):\n",
    "                for i_ch in range(self.in_channel):\n",
    "                    for o_h in range(self.out_h):\n",
    "                        for o_w in range(self.out_w):\n",
    "                            self.Z[b,o_ch,o_h,o_w] += np.sum(self.X_pad[b,i_ch,sh*o_h:sh*o_h+kh,sw*o_w:sw*o_w+kw]*self.W[i_ch,o_ch])\n",
    "                self.Z[b,o_ch] += self.b[o_ch]\n",
    "        if self.to_nn:\n",
    "            self.Z = self.Z.reshape(self.batchsize, -1)\n",
    "            \n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dA):\n",
    "        if self.to_nn:\n",
    "            dA = dA.reshape(self.batchsize, 1, 1, -1)\n",
    "    \n",
    "        self, dA = self.optimizer.update(self, dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInitializer:\n",
    "    def __init__(self, sigma=1):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, inchannel, outchannel, kh, kw):\n",
    "        return np.random.randn(inchannel, outchannel, kh, kw)\n",
    "    \n",
    "    def b(self, outchannel):\n",
    "        return np.random.randn(outchannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSGD:\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA):\n",
    "        db = np.zeros(layer.out_channel)\n",
    "        for o_ch in range(layer.out_channel):\n",
    "            db[o_ch] = np.sum(dA[:,o_ch,:,:])\n",
    "        dW = np.zeros_like(layer.W)\n",
    "    \n",
    "        for b in range(layer.batchsize):\n",
    "            for i_ch in range(layer.in_channel):\n",
    "                for o_ch in range(layer.out_channel):\n",
    "                    for h in range(layer.kernel[0]):\n",
    "                        for w in range(layer.kernel[1]):\n",
    "#                             print(layer.X_pad.shape)\n",
    "#                             print(b,i_ch,h,h+layer.out_h,w,w+layer.out_w)\n",
    "#                             print(dA[b,o_ch].shape)\n",
    "                            dW[i_ch,o_ch,h,w] = np.sum(layer.X_pad[b,i_ch,h:h+layer.out_h,w:w+layer.out_w]*dA[b,o_ch])\n",
    "                            \n",
    "        dX = np.zeros_like(layer.X)\n",
    "        for b in range(layer.batchsize):\n",
    "            for i_ch in range(layer.in_channel):\n",
    "                for o_ch in range(layer.out_channel):\n",
    "                    for h in range(layer.out_h):\n",
    "                        for w in range(layer.out_w):\n",
    "                            dX[b,i_ch,h:h+layer.kernel[0],w:w+layer.kernel[1]] += layer.W[i_ch,o_ch] * dA[b,o_ch,h,w]\n",
    "        \n",
    "        layer.b -= self.lr * db\n",
    "        layer.W -= self.lr * dW\n",
    "        self.dX = dX\n",
    "        self.dW = dW\n",
    "        return layer, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動作テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(600, 1, 28, 28)\n",
    "y_train = y_train.reshape(600, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initializer = ConvInitializer()\n",
    "optimazer = ConvSGD()\n",
    "for _ in range(1):\n",
    "    conv = SimpleConv(initializer, optimazer, in_channel=1, out_channel=2, stride=(1,1), padding=0)\n",
    "    conv.forward(X_train)\n",
    "    #print(f'b: {conv1d.b}, W: {conv1d.W}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dA = conv.Z.copy()\n",
    "dA = conv.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diver内の動作テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[1,2,3,4]]]]).astype(float)\n",
    "w = np.array([[[[3, 5, 7]]]]).astype(float)\n",
    "b = np.array([1]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[35., 50.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer = ConvInitializer()\n",
    "optimazer = ConvSGD()\n",
    "conv = SimpleConv(initializer, optimazer, in_channel=1, out_channel=1, kernel=(1,3), stride=(1,1), padding=0)\n",
    "conv.W = w\n",
    "conv.b = b\n",
    "\n",
    "conv.forward(x)\n",
    "conv.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 30., 110., 170., 140.]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA = np.array([[[[10, 20]]]]).astype(float)\n",
    "conv.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[16., 22.]],\n",
       "\n",
       "        [[17., 23.]],\n",
       "\n",
       "        [[18., 24.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer = ConvInitializer()\n",
    "optimazer = ConvSGD()\n",
    "conv = SimpleConv(initializer, optimazer, in_channel=2, out_channel=3, kernel=(1,3), stride=(1,1), padding=0)\n",
    "\n",
    "x = np.array([[[[1, 2, 3, 4]], [[2, 3, 4, 5]]]]).astype(float)  #(1, 2, 1, 4)\n",
    "w = np.ones((2, 3, 1, 3))\n",
    "b = np.array([1, 2, 3]).astype(float)\n",
    "\n",
    "conv.W = w\n",
    "conv.b = b\n",
    "conv.forward(x)\n",
    "conv.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[21., 29.]],\n",
       "\n",
       "        [[18., 25.]],\n",
       "\n",
       "        [[18., 24.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer = ConvInitializer()\n",
    "optimizer = ConvSGD()\n",
    "conv = SimpleConv(initializer, optimizer, in_channel=2, out_channel=3, kernel=(1,3), stride=(1,1), padding=0)\n",
    "\n",
    "x = np.array([[[[1,2,3,4]], [[2,3,4,5]]]]).astype(float)  #shape:(1, 2, 1, 4)\n",
    "w = np.array([[[[1,1,2]], [[2,1,1]], [[1,1,1]]],\n",
    "             [[[2,1,1]], [[1,1,1]], [[1,1,1]]]]).astype(float)     #shape: (2, 3, 1, 3)\n",
    "b = np.array([1,2,3]).astype(float)\n",
    "conv.W = w\n",
    "conv.b = b\n",
    "\n",
    "conv.forward(x)\n",
    "conv.Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[ 31.,  51.,  71.]],\n",
       " \n",
       "         [[102., 169., 236.]],\n",
       " \n",
       "         [[164., 272., 380.]]],\n",
       " \n",
       " \n",
       "        [[[ 51.,  71.,  91.]],\n",
       " \n",
       "         [[169., 236., 303.]],\n",
       " \n",
       "         [[272., 380., 488.]]]]), array([[[[125., 230., 204., 113.]],\n",
       " \n",
       "         [[102., 206., 195., 102.]]]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ = np.array([[[[9,11]],\n",
    "                   [[32,35]],\n",
    "                   [[52,56]]]])\n",
    "dX = conv.backward(loss_)\n",
    "optimizer.dW, optimizer.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.array([[1,2,3,4],\n",
    "               [2,3,4,5]])\n",
    "w_ = np.array([[[1,1,2],[2,1,1]],\n",
    "              [[2,1,1],[1,1,1]],\n",
    "              [[1,1,1],[1,1,1]]])\n",
    "b_ = np.array([1,2,3])\n",
    "\n",
    "# フォワードの出力\n",
    "out_ = np.array([[21,29],\n",
    "                [18,25],\n",
    "                [18,24]])\n",
    "loss_ = np.array([[9,11],\n",
    "                [32,35],\n",
    "                [52,56]])\n",
    "\n",
    "# バックワードの勾配\n",
    "x_delta = np.array([[125,230,204,113],\n",
    "                    [102,206,195,102]])\n",
    "w_delta = np.array([[[31,51,71],[51,71,91]],\n",
    "                    [[102,169,236],[169,236,303]],\n",
    "                    [[164,272,380],[272,380,488]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC():\n",
    "    def __init__(self, n_nodes1, n_nodes2, activator, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.activator = activator\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "\n",
    "    def forward(self, X):  \n",
    "        self.X = X\n",
    "        self.n_batch = len(self.X)\n",
    "        self.Z = np.dot(X, self.W) + self.B\n",
    "        self.A = self.activator.forward(self.Z)\n",
    "        return self.A    \n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = self.activator.backward(dA)\n",
    "        self, dA = self.optimizer.update(self, dZ)\n",
    "        return dA\n",
    "    \n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return np.random.randn(n_nodes2)\n",
    "    \n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dZ):\n",
    "        dB = np.sum(dZ, axis=0)\n",
    "        dW = np.dot(layer.X.T, dZ)\n",
    "        dA = np.dot(dZ, layer.W.T)\n",
    "        layer.B -= self.lr * dB / layer.n_batch\n",
    "        layer.W -= self.lr * dW / layer.n_batch\n",
    "        return layer, dA\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.A = np.maximum(0, Z)\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return  dA * np.where(self.A>0, 1, 0)\n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        if Z.ndim == 2:\n",
    "            Z = Z.T\n",
    "            self.A = (np.exp(Z) / np.sum(np.exp(Z), axis=0)).T\n",
    "            return self.A\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z))\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, y):\n",
    "        return self.A - y\n",
    "\n",
    "class ScratchDeepNeuralNetworkClassifier():\n",
    "    def __init__(self, *layers, epoch=3):\n",
    "        self.epoch = epoch\n",
    "        self.n_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.loss_train = []\n",
    "        self.loss_valid = []\n",
    "\n",
    "    def train(self, X, y, X_val=None, y_val=None):\n",
    "        get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n",
    "        for _ in tqdm(range(self.epoch)):\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                fout = mini_X_train.copy()\n",
    "                for layer in self.layers:\n",
    "                    fout = layer.forward(fout)\n",
    "\n",
    "                bout = mini_y_train.copy()\n",
    "                for layer in self.layers[::-1]:\n",
    "                    bout = layer.backward(bout)\n",
    "                    \n",
    "            self.loss_train.append(self.crossentropy(mini_y_train, fout))\n",
    "            if X_val is not None:\n",
    "                y_val_pred = X_val.copy()\n",
    "                for layer in self.layers:\n",
    "                    y_val_pred = layer.forward(y_val_pred)\n",
    "                self.loss_valid.append(self.crossentropy(y_val, y_val_pred))\n",
    "        \n",
    "        \n",
    "    def crossentropy(self, y, y_pred):\n",
    "        loss = -np.mean(np.sum(y*np.log(y_pred), axis=1))\n",
    "        return loss\n",
    "    \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        out = X_test\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)[:500]\n",
    "y_train = y_train[:500]\n",
    "X_train = X_train.astype(np.float)\n",
    "X_train /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:17<00:00,  5.78s/it]\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "initializer1 = ConvInitializer()\n",
    "optimizer1 = ConvSGD(lr=0.001)\n",
    "initializer2 = SimpleInitializer(sigma=1)\n",
    "optimizer2 = SGD(lr=0.001)\n",
    "\n",
    "conv = SimpleConv(initializer1, optimizer1, in_channel=1, out_channel=1, to_nn=True)\n",
    "fc = FC(784, 10, softmax, initializer2, optimizer2)\n",
    "\n",
    "nn = ScratchDeepNeuralNetworkClassifier(conv, fc, epoch=3)\n",
    "nn.train(X_train, y_train)\n",
    "\n",
    "y_pred = nn.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(y_pred, axis=1), np.argmax(y_val, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
