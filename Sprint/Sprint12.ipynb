{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "# from tensorflow import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】スクラッチを振り返る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重みの初期化\n",
    "- エポックのループ\n",
    "- optimizerの指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重みの初期化: tf.Variableで宣言して、tf.global_variable_initializerメソッドで初期化\n",
    "- エポックのループ: for loop\n",
    "- optimizerの指定: AdamOptimizerクラスを使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names, dtype=np.float32).values\n",
    "y = pd.get_dummies(data.target, dtype=np.float32).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 10.5060, val_loss : 25.4051, acc : 0.333, val_acc : 0.000\n",
      "Epoch 1, loss : 8.1467, val_loss : 10.1863, acc : 0.333, val_acc : 0.667\n",
      "Epoch 2, loss : 3.1350, val_loss : 3.1067, acc : 0.333, val_acc : 0.333\n",
      "Epoch 3, loss : 3.7615, val_loss : 8.6068, acc : 0.667, val_acc : 0.333\n",
      "Epoch 4, loss : 1.0947, val_loss : 3.3344, acc : 0.667, val_acc : 0.667\n",
      "Epoch 5, loss : 1.2243, val_loss : 3.3966, acc : 0.667, val_acc : 0.667\n",
      "Epoch 6, loss : 0.9905, val_loss : 2.9457, acc : 0.667, val_acc : 0.667\n",
      "Epoch 7, loss : 0.6546, val_loss : 2.5096, acc : 0.667, val_acc : 0.667\n",
      "Epoch 8, loss : 0.7118, val_loss : 2.6596, acc : 0.667, val_acc : 1.000\n",
      "Epoch 9, loss : 0.3233, val_loss : 2.2167, acc : 0.667, val_acc : 1.000\n",
      "test_acc: 0.667\n",
      "Epoch 0, loss : 29.7817, val_loss : 42.7011, acc : 0.667, val_acc : 0.333\n",
      "Epoch 1, loss : 3.9112, val_loss : 12.5210, acc : 0.333, val_acc : 0.333\n",
      "Epoch 2, loss : 15.9963, val_loss : 26.5143, acc : 0.667, val_acc : 0.333\n",
      "Epoch 3, loss : 15.9284, val_loss : 25.6570, acc : 0.667, val_acc : 0.333\n",
      "Epoch 4, loss : 1.7055, val_loss : 4.4628, acc : 0.667, val_acc : 0.333\n",
      "Epoch 5, loss : 0.0000, val_loss : 0.9892, acc : 1.000, val_acc : 0.667\n",
      "Epoch 6, loss : 2.3711, val_loss : 4.4751, acc : 0.667, val_acc : 0.333\n",
      "Epoch 7, loss : 1.8557, val_loss : 2.7758, acc : 0.667, val_acc : 0.667\n",
      "Epoch 8, loss : 1.5227, val_loss : 1.9393, acc : 1.000, val_acc : 1.000\n",
      "Epoch 9, loss : 3.3135, val_loss : 6.1539, acc : 0.667, val_acc : 0.333\n",
      "test_acc: 0.667\n",
      "Epoch 0, loss : 35.3751, val_loss : 34.4617, acc : 0.000, val_acc : 0.333\n",
      "Epoch 1, loss : 18.1356, val_loss : 13.9287, acc : 0.667, val_acc : 0.333\n",
      "Epoch 2, loss : 1.2393, val_loss : 0.4687, acc : 0.667, val_acc : 1.000\n",
      "Epoch 3, loss : 2.3409, val_loss : 0.8195, acc : 0.667, val_acc : 0.667\n",
      "Epoch 4, loss : 0.0012, val_loss : 0.0745, acc : 0.667, val_acc : 0.667\n",
      "Epoch 5, loss : 0.7891, val_loss : 1.0231, acc : 0.667, val_acc : 0.667\n",
      "Epoch 6, loss : 0.0000, val_loss : 0.0311, acc : 1.000, val_acc : 0.667\n",
      "Epoch 7, loss : 0.3383, val_loss : 0.3992, acc : 0.667, val_acc : 1.000\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 0.667\n",
      "Epoch 9, loss : 0.0006, val_loss : 0.3667, acc : 1.000, val_acc : 0.667\n",
      "test_acc: 1.000\n",
      "Epoch 0, loss : 11.0885, val_loss : 5.9984, acc : 0.333, val_acc : 1.000\n",
      "Epoch 1, loss : 3.0830, val_loss : 3.1774, acc : 1.000, val_acc : 1.000\n",
      "Epoch 2, loss : 4.5769, val_loss : 3.2328, acc : 1.000, val_acc : 1.000\n",
      "Epoch 3, loss : 0.5499, val_loss : 0.5006, acc : 1.000, val_acc : 1.000\n",
      "Epoch 4, loss : 0.0000, val_loss : 0.0006, acc : 0.667, val_acc : 1.000\n",
      "Epoch 5, loss : 5.4279, val_loss : 3.2702, acc : 1.000, val_acc : 1.000\n",
      "Epoch 6, loss : 1.0094, val_loss : 0.6838, acc : 1.000, val_acc : 1.000\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.0004, acc : 0.667, val_acc : 1.000\n",
      "Epoch 8, loss : 0.0544, val_loss : 0.2856, acc : 1.000, val_acc : 1.000\n",
      "Epoch 9, loss : 1.0306, val_loss : 0.6472, acc : 1.000, val_acc : 1.000\n",
      "test_acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_input = X_train.shape[1]\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_classes = 3\n",
    "\n",
    "X = tf.placeholder('float', [None, n_input])\n",
    "Y = tf.placeholder('float', [None, n_classes])\n",
    "\n",
    "logits = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(Y), tf.argmax(tf.nn.softmax(logits)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "    n_samples, n_input = X_tr.shape\n",
    "    get_mini_batch_train = GetMiniBatch(X_tr, y_tr, batch_size=batch_size)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            total_batch  = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "                sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                total_loss += loss\n",
    "                total_acc += acc\n",
    "            total_loss /= n_samples\n",
    "            total_acc /= n_samples\n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "            print(f\"Epoch {epoch}, loss : {loss:.4f}, val_loss : {val_loss:.4f}, acc : {acc:.3f}, val_acc : {val_acc:.3f}\")\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "        print(f'test_acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../week3/train.csv')\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[['GrLivArea', 'YearBuilt']].values).astype(np.float32)\n",
    "y = scaler.fit_transform(df[['SalePrice']].values).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 11.2633, val_loss : 11.6100\n",
      "Epoch 5, loss : 2.5503, val_loss : 5.6638\n",
      "Epoch 10, loss : 1.6933, val_loss : 3.6318\n",
      "Epoch 15, loss : 1.3389, val_loss : 4.4258\n",
      "test_loss: 3.418\n",
      "Epoch 0, loss : 12.9025, val_loss : 22.6607\n",
      "Epoch 5, loss : 39.7667, val_loss : 43.1913\n",
      "Epoch 10, loss : 10.4154, val_loss : 15.9734\n",
      "Epoch 15, loss : 34.5232, val_loss : 44.5793\n",
      "test_loss: 6.959\n",
      "Epoch 0, loss : 9.1323, val_loss : 28.0984\n",
      "Epoch 5, loss : 4.2526, val_loss : 8.9573\n",
      "Epoch 10, loss : 3.8737, val_loss : 15.0859\n",
      "Epoch 15, loss : 2.6550, val_loss : 10.6401\n",
      "test_loss: 11.831\n",
      "Epoch 0, loss : 15.6505, val_loss : 26.6835\n",
      "Epoch 5, loss : 5.3612, val_loss : 14.9287\n",
      "Epoch 10, loss : 4.8944, val_loss : 9.8719\n",
      "Epoch 15, loss : 6.5265, val_loss : 19.0731\n",
      "test_loss: 6.343\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.0000001\n",
    "batch_size = 10\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "X = tf.placeholder('float', [None, n_input])\n",
    "Y = tf.placeholder('float', [None, 1])\n",
    "\n",
    "output = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.square(Y - output))\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "    n_samples, n_input = X_tr.shape\n",
    "    get_mini_batch_train = GetMiniBatch(X_tr, y_tr, batch_size=batch_size)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            total_batch  = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "                sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                total_loss += loss\n",
    "                \n",
    "            total_loss /= n_samples\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "                print(f\"Epoch {epoch}, loss : {loss:.4f}, val_loss : {val_loss:.4f}\")\n",
    "\n",
    "        test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "        print(f'test_loss: {test_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)[:5000]\n",
    "y_train = y_train[:5000]\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x):\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([256, 120])),\n",
    "        'w2': tf.Variable(tf.random_normal([120, 84])),\n",
    "        'w3': tf.Variable(tf.random_normal([84, 10])),\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([120])),\n",
    "        'b2': tf.Variable(tf.random_normal([84])),\n",
    "        'b3': tf.Variable(tf.random_normal([10])),\n",
    "    }\n",
    "    c1 = tf.get_variable('chan1', [5, 5, 1, 6], dtype=tf.float32)\n",
    "    c2 = tf.get_variable('chan2', [5, 5, 6, 16], dtype=tf.float32)\n",
    "    \n",
    "    layer_1 = tf.nn.conv2d(x, filters=c1, strides=1, padding='VALID', data_format='NHWC')\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.max_pool2d(layer_1, ksize=2, strides=2, padding='VALID', data_format='NHWC')\n",
    "    layer_2 = tf.nn.conv2d(layer_1, filters=c2, strides=1, padding='VALID', data_format='NHWC')\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.max_pool2d(layer_2, ksize=2, strides=2, padding='VALID', data_format='NHWC')\n",
    "    layer_3 = tf.reshape(layer_2, [-1, 256])\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['w1']), biases['b1'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    layer_5 = tf.add(tf.matmul(layer_4, weights['w2']), biases['b2'])\n",
    "    layer_5 = tf.nn.relu(layer_5)    \n",
    "    layer_output = tf.add(tf.matmul(layer_5, weights['w3']), biases['b3'])\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.8832, val_loss : 2.8641, acc : 0.500, val_acc : 0.000\n",
      "Epoch 5, loss : 0.0082, val_loss : 1.0406, acc : 0.300, val_acc : 0.300\n",
      "Epoch 10, loss : 0.0107, val_loss : 0.8099, acc : 0.500, val_acc : 0.500\n",
      "Epoch 15, loss : 0.0000, val_loss : 0.7965, acc : 0.400, val_acc : 0.900\n",
      "Epoch 20, loss : 0.0000, val_loss : 1.1591, acc : 0.600, val_acc : 0.800\n",
      "Epoch 25, loss : 0.0000, val_loss : 0.8695, acc : 0.600, val_acc : 1.000\n",
      "test_acc: 1.000\n",
      "Epoch 0, loss : 9.3236, val_loss : 5.2649, acc : 0.300, val_acc : 0.300\n",
      "Epoch 5, loss : 3.0409, val_loss : 2.1967, acc : 0.700, val_acc : 0.500\n",
      "Epoch 10, loss : 0.4478, val_loss : 1.2594, acc : 0.600, val_acc : 0.600\n",
      "Epoch 15, loss : 0.0002, val_loss : 1.5522, acc : 0.700, val_acc : 0.600\n",
      "Epoch 20, loss : 0.0015, val_loss : 0.9612, acc : 0.700, val_acc : 0.700\n",
      "Epoch 25, loss : 0.0000, val_loss : 1.0686, acc : 0.700, val_acc : 0.700\n",
      "test_acc: 0.800\n",
      "Epoch 0, loss : 4.7750, val_loss : 4.7630, acc : 0.500, val_acc : 0.300\n",
      "Epoch 5, loss : 0.0398, val_loss : 1.7995, acc : 0.600, val_acc : 0.100\n",
      "Epoch 10, loss : 0.0001, val_loss : 1.1319, acc : 0.600, val_acc : 0.400\n",
      "Epoch 15, loss : 0.0166, val_loss : 0.9648, acc : 0.800, val_acc : 0.400\n",
      "Epoch 20, loss : 0.0048, val_loss : 0.9423, acc : 0.700, val_acc : 0.800\n",
      "Epoch 25, loss : 0.0010, val_loss : 1.1246, acc : 0.700, val_acc : 0.800\n",
      "test_acc: 0.700\n",
      "Epoch 0, loss : 3.6688, val_loss : 4.8442, acc : 0.400, val_acc : 0.200\n",
      "Epoch 5, loss : 0.0529, val_loss : 1.5278, acc : 0.700, val_acc : 0.400\n",
      "Epoch 10, loss : 0.0097, val_loss : 1.1539, acc : 0.700, val_acc : 0.500\n",
      "Epoch 15, loss : 0.0000, val_loss : 1.0175, acc : 0.800, val_acc : 0.500\n",
      "Epoch 20, loss : 0.0000, val_loss : 1.2786, acc : 0.700, val_acc : 0.500\n",
      "Epoch 25, loss : 0.0000, val_loss : 1.0816, acc : 0.800, val_acc : 0.700\n",
      "test_acc: 0.600\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "batch = 5\n",
    "\n",
    "X = tf.placeholder('float', [None, 28, 28, 1])\n",
    "Y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "logits = net(X)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "correct_pred = tf.equal(tf.argmax(Y), tf.argmax(tf.nn.softmax(logits)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "    n_samples = X_tr.shape[0]\n",
    "    get_mini_batch_train = GetMiniBatch(X_tr, y_tr, batch_size=batch_size)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            total_batch  = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "                sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "                total_loss += loss\n",
    "                total_acc += acc\n",
    "            total_loss /= n_samples\n",
    "            total_acc /= n_samples\n",
    "            \n",
    "            if epoch%5 == 0:\n",
    "                val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "                print(f\"Epoch {epoch}, loss : {loss:.4f}, val_loss : {val_loss:.4f}, acc : {acc:.3f}, val_acc : {val_acc:.3f}\")\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "        print(f'test_acc: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
